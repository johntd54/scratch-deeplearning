{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CONVOLUTIONS</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dawnet.utils.image import show_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transposed convolution\n",
    "\n",
    "Transposed convolution is a method to \"reconstruct\" the input feature map, using the knowledge of output feature map and the convolution filters. I put the reconstruct inside a quote because this reconstruction merely recreates an input feature map that has the same dimension as the original feature map, while the exact values of the original feature map are not reconstructed.\n",
    "\n",
    "Even though the transposed convolution does not faithfully reconstruct the input feature map, it is still useful for the following reasons:\n",
    "- the input feature map shape is retained, hence transposed convolution provides a mechanism to upsample an image\n",
    "- the exact reconstructed values are not the same as the original values, however, since both the knowlege of output feature map and the convolution filters are used, the values still provide some knowledge regarding the input feature map (the values of reconstructed input feature map depends on the values of original feature map)\n",
    "- it is fast to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the input feature map simply as list of 16 numbers, reshaped as a 4x4 matrix, reformatted to have a batch and channel dimension in order to work with Pytorch `nn.Conv2d`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature:\n",
      "tensor([[[[76., 97., 21., 93., 75., 17., 66., 95., 79., 59.],\n",
      "          [ 6., 67., 36., 42., 64.,  3., 61., 62., 87., 58.],\n",
      "          [86., 89., 98., 70., 56., 83., 22., 45., 12., 85.],\n",
      "          [74., 73., 57., 82., 37., 38., 19., 20.,  7., 35.],\n",
      "          [53., 77., 25., 71.,  4., 88., 90., 44., 91., 15.],\n",
      "          [80., 39., 46., 34., 16., 33., 84., 81., 65., 69.],\n",
      "          [ 0., 43., 72., 49., 14., 99., 40., 10., 24., 63.],\n",
      "          [ 5., 54., 96., 27., 55., 48., 78., 13., 30., 51.],\n",
      "          [11., 60., 31.,  2.,  8., 26., 68., 47., 52., 50.],\n",
      "          [28., 94., 18., 23., 32., 41., 92., 29.,  1.,  9.]]]])\n",
      "\n",
      "Output_feature:\n",
      "tensor([[[[ 69.6557,  32.1158,  71.9167,  45.9923,  -1.2418,  78.3735,  36.2720,\n",
      "            53.6632],\n",
      "          [ 58.0597,  45.7470,  37.2921,  26.4294,  24.7348,  31.6862,  37.8080,\n",
      "            22.5889],\n",
      "          [ 46.3977,  49.6880,  23.9288,  47.3108,  44.2560,  17.2110,  51.6113,\n",
      "            19.8462],\n",
      "          [ 52.4065,  19.7753,  45.9308,  -9.1038,  52.2293,  37.6946,  22.0879,\n",
      "            58.2667],\n",
      "          [ 27.7577,  29.5851,  28.7646,  36.9888,  62.2145,  42.0189,  28.4958,\n",
      "            36.3077],\n",
      "          [ 37.1879,  37.6372,  31.4955,   7.8226,  85.1850,  40.3114,  29.1560,\n",
      "            30.6597],\n",
      "          [ 50.6000,  48.8651, -11.4751,  33.5827,  53.8918,  18.3085,   9.5403,\n",
      "            45.4534],\n",
      "          [ 64.4817,  36.9075,   0.8739,  36.0048,  51.5789,  38.9113,  -0.0386,\n",
      "            28.8391]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "WIDTH = 10\n",
    "HEIGHT = 10\n",
    "RANDOM = True        # flip this value between True/False to see the dependence of reconstructed on original\n",
    "\n",
    "input_feature = np.arange(HEIGHT * WIDTH).astype(np.float32)\n",
    "if RANDOM:\n",
    "    np.random.shuffle(input_feature)\n",
    "input_feature = input_feature.reshape(1, 1, HEIGHT, WIDTH)\n",
    "input_feature = torch.FloatTensor(input_feature)\n",
    "\n",
    "conv_forward = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "output_feature = conv_forward(input_feature)\n",
    "\n",
    "print('Input feature:')\n",
    "print(input_feature)\n",
    "print()\n",
    "print('Output_feature:')\n",
    "print(output_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the weights and biases of the forward convolution filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "tensor([[[[-0.2918,  0.2762,  0.2772],\n",
      "          [-0.1903,  0.3308, -0.0957],\n",
      "          [ 0.1388,  0.0036,  0.2998]]]])\n",
      "Biases:\n",
      "tensor([0.0138])\n"
     ]
    }
   ],
   "source": [
    "print('Weights:')\n",
    "print(conv_forward.weight.data)\n",
    "print('Biases:')\n",
    "print(conv_forward.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a transposed convolution layer that has the same configurations with `conv_forward`. Then re-initialized the layer's weights and biases with those of `conv_forward`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (before):\n",
      "tensor([[[[-0.2611,  0.0115, -0.0957],\n",
      "          [-0.1362,  0.2726, -0.0306],\n",
      "          [ 0.0339,  0.0668, -0.1967]]]])\n",
      "Biases (before):\n",
      "tensor([0.1718])\n",
      "\n",
      "Weights (after):\n",
      "tensor([[[[-0.2918,  0.2762,  0.2772],\n",
      "          [-0.1903,  0.3308, -0.0957],\n",
      "          [ 0.1388,  0.0036,  0.2998]]]])\n",
      "Biases (after):\n",
      "tensor([0.0138])\n"
     ]
    }
   ],
   "source": [
    "conv_transposed = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "print('Weights (before):')\n",
    "print(conv_transposed.weight.data)\n",
    "print('Biases (before):')\n",
    "print(conv_transposed.bias.data)\n",
    "\n",
    "_ = conv_transposed.weight.data.copy_(conv_forward.weight.data)\n",
    "_ = conv_transposed.bias.data.copy_(conv_forward.bias.data)\n",
    "\n",
    "print()\n",
    "print('Weights (after):')\n",
    "print(conv_transposed.weight.data)\n",
    "print('Biases (after):')\n",
    "print(conv_transposed.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the transposed convolution to get the reconstructed input feature map. Normalize it to have the same range with the original input feature map and compare it with the original input feature map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed input feature:\n",
      "tensor([[[[-20.3127,   9.8792,   7.2045,  15.3562,  33.0122, -10.4513,  10.7298,\n",
      "            16.0954,  24.8883,  14.8884],\n",
      "          [-30.1812,  19.6277,   8.1293,  27.2424,  18.9944, -14.8011,  23.7284,\n",
      "             6.9299,  31.0086,   1.1369],\n",
      "          [-14.9029,  13.5352,  53.0726,  25.7853,  28.8169,  44.6134,   7.8350,\n",
      "            49.4966,  34.7188,  19.4409],\n",
      "          [-16.0473,  21.1672,  36.7850,  32.5103,  14.6354,  20.1593,  22.8688,\n",
      "            23.9769,  35.2668,  21.0366],\n",
      "          [-11.6164,  19.6810,  17.6788,  41.9449,  -3.8121,  42.8939,  44.3065,\n",
      "             9.6460,  50.6416,  10.4489],\n",
      "          [ -8.8446,   5.7859,  35.3389,  21.3352,   4.6776,  25.6763,  47.6301,\n",
      "            28.2552,  32.6741,  22.5049],\n",
      "          [-17.9741,   9.0742,  46.1999,  20.0220,  -8.8615,  55.7652,  39.6124,\n",
      "            12.1497,  31.2339,  20.5627],\n",
      "          [-23.2677,  19.8488,  56.9817,  -2.4023,  18.3967,  32.2556,  53.8796,\n",
      "            11.5738,  30.9393,  12.8477],\n",
      "          [ -5.2304,  21.2837,  19.6334,   9.1899,   6.1862,  19.0261,  25.5000,\n",
      "             2.6212,  12.5791,  10.8800],\n",
      "          [  8.9645,   5.3676,  19.5996,  16.0801,   7.5643,  16.3943,  15.6117,\n",
      "            15.6830,   0.1054,   8.6602]]]],\n",
      "       grad_fn=<ThnnConvTranspose2DBackward>)\n",
      "\n",
      "Normalized reconstructed input feature:\n",
      "tensor([[[[11.2087, 45.5007, 42.4628, 51.7216, 71.7754, 22.4093, 46.4669,\n",
      "           52.5611, 62.5481, 51.1902],\n",
      "          [ 0.0000, 56.5732, 43.5133, 65.2220, 55.8538, 17.4688, 61.2307,\n",
      "           42.1510, 69.4996, 35.5713],\n",
      "          [17.3531, 49.6532, 94.5600, 63.5670, 67.0102, 84.9520, 43.1790,\n",
      "           90.4984, 73.7136, 56.3610],\n",
      "          [16.0534, 58.3217, 76.0605, 71.2052, 50.9029, 57.1770, 60.2544,\n",
      "           61.5130, 74.3361, 58.1734],\n",
      "          [21.0859, 56.6337, 54.3596, 81.9212, 29.9502, 82.9990, 84.6035,\n",
      "           45.2359, 91.7989, 46.1479],\n",
      "          [24.2342, 40.8515, 74.4181, 58.5126, 39.5928, 63.4432, 88.3784,\n",
      "           66.3724, 71.3913, 59.8411],\n",
      "          [13.8648, 44.5864, 86.7540, 57.0210, 24.2151, 97.6183, 79.2719,\n",
      "           48.0797, 69.7555, 57.6351],\n",
      "          [ 7.8523, 56.8243, 99.0000, 31.5514, 55.1750, 70.9160, 95.4767,\n",
      "           47.4256, 69.4210, 48.8724],\n",
      "          [28.3392, 58.4540, 56.5796, 44.7179, 41.3063, 55.8898, 63.2429,\n",
      "           37.2571, 48.5674, 46.6375],\n",
      "          [44.4619, 40.3764, 56.5412, 52.5438, 42.8715, 52.9006, 52.0117,\n",
      "           52.0927, 34.3997, 44.1162]]]], grad_fn=<ThMulBackward>)\n",
      "\n",
      "Original input feature:\n",
      "tensor([[[[76., 97., 21., 93., 75., 17., 66., 95., 79., 59.],\n",
      "          [ 6., 67., 36., 42., 64.,  3., 61., 62., 87., 58.],\n",
      "          [86., 89., 98., 70., 56., 83., 22., 45., 12., 85.],\n",
      "          [74., 73., 57., 82., 37., 38., 19., 20.,  7., 35.],\n",
      "          [53., 77., 25., 71.,  4., 88., 90., 44., 91., 15.],\n",
      "          [80., 39., 46., 34., 16., 33., 84., 81., 65., 69.],\n",
      "          [ 0., 43., 72., 49., 14., 99., 40., 10., 24., 63.],\n",
      "          [ 5., 54., 96., 27., 55., 48., 78., 13., 30., 51.],\n",
      "          [11., 60., 31.,  2.,  8., 26., 68., 47., 52., 50.],\n",
      "          [28., 94., 18., 23., 32., 41., 92., 29.,  1.,  9.]]]])\n",
      "tensor([[[[66.5615, 36.4724, 63.1916, 49.8805,  9.9773, 72.8488, 42.3629,\n",
      "           57.4246],\n",
      "          [57.0076, 59.0597, 43.5030, 30.3932, 42.0619, 39.5589, 58.5818,\n",
      "           44.7800],\n",
      "          [63.2540, 69.1196, 29.5745, 57.6954, 49.1667, 40.7808, 74.0318,\n",
      "           37.3425],\n",
      "          [67.8559, 46.5784, 47.8886, 22.7998, 63.5254, 53.9027, 44.0093,\n",
      "           63.4657],\n",
      "          [54.4389, 56.0418, 35.9875, 40.5431, 70.1135, 50.7527, 43.6558,\n",
      "           54.4163],\n",
      "          [59.6291, 57.3173, 35.8651, 25.1114, 87.1069, 51.7533, 40.7205,\n",
      "           46.8391],\n",
      "          [61.2564, 67.4036,  3.8731, 45.6882, 70.5917, 39.4481, 23.8422,\n",
      "           49.8106],\n",
      "          [72.6499, 44.3900, 16.0925, 48.8117, 56.2685, 48.6980, 17.8525,\n",
      "           43.9841]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_feature_reconstructed = conv_transposed(output_feature)\n",
    "\n",
    "print('Reconstructed input feature:')\n",
    "print(input_feature_reconstructed)\n",
    "print()\n",
    "print('Normalized reconstructed input feature:')\n",
    "input_feature_reconstructed -= torch.min(input_feature_reconstructed)\n",
    "input_feature_reconstructed *= torch.max(input_feature) / torch.max(input_feature_reconstructed)\n",
    "print(input_feature_reconstructed)\n",
    "print()\n",
    "print('Original input feature:')\n",
    "print(input_feature)\n",
    "\n",
    "print(conv_forward(input_feature_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The image of original feature map and reconstructed feature map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_feature_np = input_feature.squeeze().cpu().numpy()\n",
    "input_feature_reconstructed_np = input_feature_reconstructed.squeeze().cpu().detach().numpy()\n",
    "\n",
    "show_images([input_feature_np, input_feature_reconstructed_np], ['Original', 'Reconstructed'],\n",
    "            max_columns=2, notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
